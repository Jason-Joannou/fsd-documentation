{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Group 2 FSD Documentation","text":""},{"location":"#project-overview","title":"Project Overview","text":""},{"location":"#business-context","title":"Business Context","text":"<p>Blockchain technology has garnered significant attention since its first application with Bitcoin in 2009. The journey of Bitcoin, from a price as low as $1 in 2011 to a peak of around $61,000 in 2022, has been remarkable. This surge in Bitcoin's value has paved the way for numerous alternative cryptocurrencies, known as altcoins, which aim to enhance transaction speeds and introduce diverse use cases for blockchain technology. Despite the proliferation of these altcoins, the cryptocurrency market remains highly volatile and closely linked to Bitcoin's performance.</p>"},{"location":"#why-is-the-project-important","title":"Why is the Project Important?","text":"<p>In the rapidly evolving world of cryptocurrencies, financial literacy and transparency are paramount. A financial crypto dashboard serves as a crucial tool for both novice and experienced investors by providing:</p> <ul> <li> <p>Financial Literacy: Educates users about the dynamics of the cryptocurrency market, helping them make informed investment decisions.</p> </li> <li> <p>Transparency: Offers clear and concise information about market trends, volatility, and the performance of various cryptocurrencies.</p> </li> <li> <p>Risk Management: Helps investors assess risks by providing key indicators and analytics, thereby promoting more stable and strategic investment choices.</p> </li> </ul>"},{"location":"#opportunities","title":"Opportunities","text":"<ol> <li>Education and Empowerment: By providing a user-friendly dashboard, we can educate and empower users to make smarter investment choices.</li> <li>Market Insights: Offering detailed market analysis and volatility indicators can help investors navigate the market's uncertainties.</li> <li>Adoption and Growth: A well-designed dashboard can drive the adoption of cryptocurrencies by making the market more accessible and understandable.</li> </ol>"},{"location":"#problems","title":"Problems","text":"<ol> <li>Volatility: The cryptocurrency market is notoriously volatile, making it challenging for investors to make stable and informed decisions.</li> <li>Lack of Information: Many investors lack access to comprehensive and reliable data on market trends and the performance of different cryptocurrencies.</li> <li>Complexity: The rapid development and complex nature of blockchain technology and cryptocurrencies can be overwhelming for new investors.</li> </ol>"},{"location":"#vision","title":"Vision","text":"<p>Our vision is to create a comprehensive financial crypto dashboard that bridges the gap between complex market data and user-friendly insights. This platform will not only provide data analytics but also foster an informed and empowered user base. By enhancing financial literacy and transparency, we aim to support the growth and stability of the cryptocurrency market, ultimately contributing to a more inclusive and robust financial ecosystem.</p>"},{"location":"#fsd-public-api","title":"FSD Public API","text":"<p>While our primary project focuses on delivering a robust analytical engine and an interactive dashboard for in-depth market analysis, the public API was developed separately to address specific needs and opportunities for broader access and usability. For more information on our public API you can visit its documentation page here</p>"},{"location":"api/endpoints/","title":"Endpoints","text":""},{"location":"api/endpoints/#1-root-endpoint","title":"1. Root Endpoint","text":"<ul> <li>URL: <code>/</code></li> <li>Method: <code>GET</code></li> <li>Description: Tests the connection to the API.</li> <li>Response:</li> <li><code>200 OK</code>: Connection is successful.</li> </ul>"},{"location":"api/endpoints/#2-get-coin-names","title":"2. Get Coin Names","text":"<ul> <li>URL: <code>/get_coin_names</code></li> <li>Method: <code>GET</code></li> <li>Description: Retrieves a list of distinct cryptocurrency names from the <code>CoinsTable</code>.</li> <li>Response:</li> <li><code>200 OK</code>: Returns a JSON object with the coin names.</li> <li><code>400 Bad Request</code>: Error message if the query fails.</li> </ul>"},{"location":"api/endpoints/#3-query-coin-data","title":"3. Query Coin Data","text":"<ul> <li>URL: <code>/query_coin</code></li> <li>Method: <code>GET</code></li> <li>Description: Retrieves data for specific coins from the <code>CoinsTable</code>.</li> <li>Parameters:</li> <li><code>coin_names</code> (List of strings, required): List of coin names to query.</li> <li>Response:</li> <li><code>200 OK</code>: Returns the queried coin data.</li> <li><code>500 Internal Server Error</code>: Error message with details if the query fails.</li> </ul>"},{"location":"api/endpoints/#4-daily-price-change","title":"4. Daily Price Change","text":"<ul> <li>URL: <code>/daily_price_change</code></li> <li>Method: <code>GET</code></li> <li>Description: Computes and plots the daily price change of selected coins over a specified date range.</li> <li>Parameters:</li> <li><code>coin_names</code> (List of strings, required): List of coin names.</li> <li><code>start_date</code> (string, required): Start date in the format <code>YYYY-MM-DD</code>.</li> <li><code>end_date</code> (string, required): End date in the format <code>YYYY-MM-DD</code>.</li> <li>Response:</li> <li><code>200 OK</code>: Returns a JSON object with a Plotly graph of daily price changes.</li> <li><code>400 Bad Request</code>: Error message if the query fails.</li> </ul>"},{"location":"api/endpoints/#5-daily-price-range","title":"5. Daily Price Range","text":"<ul> <li>URL: <code>/daily_price_range</code></li> <li>Method: <code>GET</code></li> <li>Description: Computes and plots the daily price range of selected coins over a specified date range.</li> <li>Parameters:</li> <li><code>coin_names</code> (List of strings, required): List of coin names.</li> <li><code>start_date</code> (string, required): Start date in the format <code>YYYY-MM-DD</code>.</li> <li><code>end_date</code> (string, required): End date in the format <code>YYYY-MM-DD</code>.</li> <li>Response:</li> <li><code>200 OK</code>: Returns a JSON object with a Plotly graph of daily price ranges.</li> <li><code>400 Bad Request</code>: Error message if the query fails.</li> </ul>"},{"location":"api/endpoints/#6-moving-averages","title":"6. Moving Averages","text":"<ul> <li>URL: <code>/moving_averages</code></li> <li>Method: <code>GET</code></li> <li>Description: Computes and plots the moving averages for selected coins over a specified date range.</li> <li>Parameters:</li> <li><code>coin_names</code> (List of strings, required): List of coin names.</li> <li><code>start_date</code> (string, required): Start date in the format <code>YYYY-MM-DD</code>.</li> <li><code>end_date</code> (string, required): End date in the format <code>YYYY-MM-DD</code>.</li> <li><code>window</code> (integer, optional, default=5): The window size for calculating the moving average.</li> <li>Response:</li> <li><code>200 OK</code>: Returns a JSON object with a Plotly graph of moving averages.</li> <li><code>400 Bad Request</code>: Error message if the query fails.</li> </ul>"},{"location":"api/endpoints/#7-correlation-analysis","title":"7. Correlation Analysis","text":"<ul> <li>URL: <code>/correlation_analysis</code></li> <li>Method: <code>GET</code></li> <li>Description: Performs correlation analysis on the selected coins over a specified date range and returns a heatmap visualization.</li> <li>Parameters:</li> <li><code>coin_names</code> (List of strings, required): List of coin names.</li> <li><code>start_date</code> (string, required): Start date in the format <code>YYYY-MM-DD</code>.</li> <li><code>end_date</code> (string, required): End date in the format <code>YYYY-MM-DD</code>.</li> <li>Response:</li> <li><code>200 OK</code>: Returns a JSON object with a Plotly heatmap of correlation analysis.</li> <li><code>400 Bad Request</code>: Error message if the query fails.</li> </ul>"},{"location":"api/endpoints/#8-coin-reporting","title":"8. Coin Reporting","text":"<ul> <li>URL: <code>/coin_reporting</code></li> <li>Method: <code>GET</code></li> <li>Description: Generates a report with various visualizations (e.g., boxplots) for a specific cryptocurrency.</li> <li>Parameters:</li> <li><code>coin_name</code> (string, optional, default=\"Bitcoin\"): The name of the cryptocurrency to report on.</li> <li>Response:</li> <li><code>200 OK</code>: Returns a JSON object with Plotly visualizations for the selected coin.</li> <li><code>400 Bad Request</code>: Error message if the query fails.</li> </ul>"},{"location":"api/endpoints/#9-coin-proportions","title":"9. Coin Proportions","text":"<ul> <li>URL: <code>/coin_proportion</code></li> <li>Method: <code>GET</code></li> <li>Description: Generates a summary report showing the proportions of different coins in the dataset, along with a pie chart visualization.</li> <li>Response:</li> <li><code>200 OK</code>: Returns a JSON object with a pie chart and summary table visualizations.</li> <li><code>400 Bad Request</code>: Error message if the query fails.</li> </ul>"},{"location":"api/endpoints/#10-user-registration","title":"10. User Registration","text":"<ul> <li>URL: <code>/register/</code></li> <li>Method: <code>POST</code></li> <li>Description: Registers a new user in the system.</li> <li>Request Body:</li> <li><code>username</code> (string, required): The desired username.</li> <li><code>email</code> (string, required): The user's email address.</li> <li><code>password</code> (string, required): The desired password.</li> <li>Response:</li> <li><code>200 OK</code>: Registration successful.</li> <li><code>400 Bad Request</code>: Error message if the username or email already exists or if registration fails.</li> </ul>"},{"location":"api/endpoints/#11-user-login","title":"11. User Login","text":"<ul> <li>URL: <code>/login/</code></li> <li>Method: <code>POST</code></li> <li>Description: Authenticates a user and logs them into the system.</li> <li>Request Body:</li> <li><code>username</code> (string, required): The username.</li> <li><code>password</code> (string, required): The user's password.</li> <li>Response:</li> <li><code>200 OK</code>: Login successful.</li> <li><code>401 Unauthorized</code>: Invalid username or password.</li> </ul>"},{"location":"api/endpoints/#12-update-username","title":"12. Update Username","text":"<ul> <li>URL: <code>/update_username/</code></li> <li>Method: <code>POST</code></li> <li>Description: Updates the username of an existing user.</li> <li>Request Body:</li> <li><code>old_username</code> (string, required): The current username.</li> <li><code>new_username</code> (string, required): The new username.</li> <li>Response:</li> <li><code>200 OK</code>: Username updated successfully.</li> <li><code>400 Bad Request</code>: Error message if the update fails.</li> </ul>"},{"location":"api/endpoints/#13-update-user-email","title":"13. Update User Email","text":"<ul> <li>URL: <code>/update_useremail/</code></li> <li>Method: <code>POST</code></li> <li>Description: Updates the email address of an existing user.</li> <li>Request Body:</li> <li><code>username</code> (string, required): The current username.</li> <li><code>new_email</code> (string, required): The new email address.</li> <li>Response:</li> <li><code>200 OK</code>: Email updated successfully.</li> <li><code>400 Bad Request</code>: Error message if the update fails.</li> </ul>"},{"location":"api/endpoints/#14-update-password","title":"14. Update Password","text":"<ul> <li>URL: <code>/update_password/</code></li> <li>Method: <code>POST</code></li> <li>Description: Updates the password of an existing user.</li> <li>Request Body:</li> <li><code>username</code> (string, required): The current username.</li> <li><code>new_password</code> (string, required): The new password.</li> <li><code>current_password</code> (string, required): The current password for verification.</li> <li>Response:</li> <li><code>200 OK</code>: Password updated successfully.</li> <li><code>400 Bad Request</code>: Error message if the update fails.</li> </ul>"},{"location":"api/endpoints/#error-handling","title":"Error Handling","text":"<p>Each endpoint can return the following error responses:</p> <ul> <li><code>400 Bad Request</code>: If the input is invalid or if the operation fails.</li> <li><code>401 Unauthorized</code>: If the login credentials are incorrect.</li> <li><code>500 Internal Server Error</code>: If there is a server-side issue.</li> </ul>"},{"location":"api/endpoints/#dependencies","title":"Dependencies","text":"<ul> <li><code>FastAPI</code>: The web framework used to build this API.</li> <li><code>SQLAlchemy</code>: Used for database interactions.</li> <li><code>Plotly</code>: Used for generating visualizations.</li> <li><code>bcrypt</code>: Used for hashing passwords.</li> <li><code>SQLite</code>: The database used for storing user and cryptocurrency data.</li> </ul>"},{"location":"api/endpoints/#middleware","title":"Middleware","text":"<ul> <li>CORS: Cross-Origin Resource Sharing is enabled with all origins allowed for development purposes.</li> </ul>"},{"location":"api/overview/","title":"Overview","text":"<p>Our API serves as a robust interface for accessing and analyzing cryptocurrency data. It provides endpoints for retrieving various types of financial information, such as daily price changes, price ranges, and moving averages. Additionally, it supports advanced analytical functions, including correlation analysis between different cryptocurrencies and detailed coin reporting. The API also integrates user authentication and management features to ensure secure and personalized access. By facilitating data retrieval, analysis, and visualization, this API empowers users with the tools needed to make informed investment decisions and gain deeper insights into the cryptocurrency market.</p>"},{"location":"architecture/components/","title":"Components","text":""},{"location":"architecture/components/#database-architecture","title":"Database Architecture","text":"<p>The database design is centered around two primary tables: </p> <ul> <li>The Coins table</li> <li>The Users table</li> </ul> <p>The Coins table stores essential information about various cryptocurrencies, including their names, symbols, and historical data. The Users table manages user authentication and stores sign-in credentials, ensuring secure access to the application. This design enables efficient data management and supports the core functionalities of the crypto financial dashboard.</p> <p>The image below shows the structure of the two tables that are used in the project architecture.</p> <p></p>"},{"location":"architecture/components/#backend-architecture","title":"Backend Architecture","text":"<p>The API serves as the bridge between the frontend and the database, built using FastAPI, a modern web framework for creating APIs with Python. It manages data processing, business logic, and database interactions, offering endpoints for data retrieval, updates, and analytical operations. The API is containerized and deployed on Azure Container Instances, facilitating easy scaling and management.</p>"},{"location":"architecture/components/#architecture-diagram","title":"Architecture Diagram","text":""},{"location":"architecture/components/#example-process-flow","title":"Example Process Flow","text":"<p>API Request/Route</p> <ul> <li>Example: <code>http://127.0.0.1:8000/moving_averages?coin_name=Bitcoin&amp;coin_name=Cardano</code></li> </ul> <p>When a user requests moving averages for specific coins from the frontend, an API request is sent to the corresponding route. The API then queries the database for the relevant data, processes it through the analytics and visualization modules, and finally returns a JSON response containing the transaction status, message, and processed data.</p> <p>Database SQL Request</p> <ul> <li>Example: <code>SELECT * FROM CoinsTable WHERE NAME IN ('Bitcoin', 'Cardano')</code></li> </ul> <p>Within the API route, a dynamic SQL query is generated based on the client\u2019s request to fetch data from the database. The retrieved data is loaded into a Pandas DataFrame, which is then passed to the analytics module for further processing.</p> <p>Analytics Request</p> <ul> <li>Example: <code>moving_average(df: pd.DataFrame, window: int = 5) -&gt; pd.DataFrame</code></li> </ul> <p>The analytics module processes the DataFrame, applying the requested analytical function\u2014such as calculating moving averages. Once the computation is complete, the DataFrame is returned, ready for visualization.</p> <p>Visualization Request</p> <ul> <li>Example: <code>plot_line(df: pd.DataFrame, x_column_name: str, y_column_name: str) -&gt; go.Figure</code></li> </ul> <p>The visualization module contains generic functions for generating various plots. After the data is processed by the analytics module, it is passed to the visualization module, along with the necessary columns and metadata. The visualization function returns a Plotly figure object, which is converted to JSON and included in the API response.</p> <p>API Response</p> <ul> <li>Example: <code>{\"transaction_state\": 200, \"figure_data\": {...}}</code></li> </ul> <p>After all processing is complete, the API sends a response to the frontend, including the transaction status and the visualized data, ready for display.</p>"},{"location":"architecture/components/#frontend-architecture","title":"Frontend Architecture","text":"<p>The frontend of our crypto financial dashboard is built using Vue.js, a progressive JavaScript framework that facilitates the creation of dynamic and interactive web applications. The architecture of the frontend is designed to ensure a seamless and responsive user experience, focusing on modularity, reusability, and efficient data handling.</p>"},{"location":"architecture/components/#key-components","title":"Key Components","text":"<p>Component-Based Structure</p> <ul> <li>The frontend is organized into a series of reusable Vue components. Each component is responsible for a specific part of the user interface, such as charts, tables, and forms. This modular approach allows for easier maintenance and scalability.</li> <li>Common components include:<ul> <li>Dashboard Component: The main view that displays various financial metrics, trends, and visualizations.</li> <li>Chart Components: Custom components that render different types of charts (e.g., line charts, bar charts) using libraries like Plotly or Chart.js.</li> <li>Data Table Components: Components that display tabular data with features like sorting, filtering, and pagination.</li> <li>Form Components: Components that handle user inputs for searching, filtering, or submitting data.</li> </ul> </li> </ul> <p>State Management</p> <ul> <li>The application state is managed using Vuex, Vue's centralized state management library. Vuex allows us to store and manage the application's global state, such as user authentication, selected cryptocurrencies, and chart data.</li> <li>State management is crucial for ensuring consistency across different components and maintaining the application's responsiveness.</li> </ul> <p>Routing</p> <ul> <li>Vue Router is used to manage navigation within the dashboard. It allows users to switch between different views (e.g., dashboard, analytics, settings) without reloading the page.</li> <li>The routing structure is designed to be intuitive, with clear paths and dynamic routing for accessing specific data points or visualizations.</li> </ul> <p>Data Handling and API Integration</p> <ul> <li>The frontend communicates with the backend via RESTful API calls to fetch real-time cryptocurrency data, historical trends, and other financial metrics.</li> <li>Axios is used for making HTTP requests, ensuring smooth data retrieval and error handling.</li> <li>Data retrieved from the API is processed and visualized in various components, allowing users to interact with and explore the financial data effectively.</li> </ul> <p>Styling and Theming</p> <ul> <li>The dashboard's appearance is styled using ElementUI, a Vue.js-based UI component library that provides a rich set of pre-designed components and a consistent design system.</li> <li>ElementUI is leveraged to create a cohesive and professional look, with components like buttons, modals, forms, and tables seamlessly integrated into the dashboard.</li> <li>Theming is applied to ensure the dashboard has a professional and cohesive look.</li> </ul> <p>Performance Optimization</p> <ul> <li>Lazy Loading: Vue's lazy loading feature is employed to load components only when they are needed, improving the application's load time and performance.</li> <li>Caching: Caching strategies are implemented to reduce the number of API calls and enhance the user experience by loading data faster.</li> </ul>"},{"location":"architecture/components/#diagram","title":"Diagram","text":"<p>You can include a diagram that illustrates the relationship between these components, the flow of data, and the interaction with the backend services.</p>"},{"location":"architecture/components/#azure-workflow","title":"Azure Workflow","text":"<p>The project's Azure workflow utilizes two primary Azure resources to host the projects components: Azure Container Registry (ACR) and Azure Container Instance (ACI). This workflow is anchored in Docker, which is used to create, build, and deploy images to Azure for hosting. Below is an expanded explanation of ACR, ACI, and the Docker workflow involved in the project - for further reading on docker builds and the azure resource management through terraform you can refer to the Docker documentation and the Terraform documentation of the project.</p>"},{"location":"architecture/components/#azure-container-registry-acr","title":"Azure Container Registry (ACR)","text":"<p>Azure Container Registry (ACR) is a managed, private Docker registry service provided by Microsoft Azure. It allows you to store and manage container images (such as Docker images) for all types of container deployments.</p> <p>Key Features of ACR:</p> <ul> <li>Private Registry: Securely store your Docker images and other OCI artifacts.</li> <li>Scalability: Provisioning scales seamlessly to accommodate growing needs.</li> <li>Integration: Easily integrates with Azure Kubernetes Service (AKS), Azure DevOps, and other container orchestrators.</li> <li>Geo-replication: Replicate your registry across multiple Azure regions for high availability and disaster recovery.</li> <li>Vulnerability Scanning: Ensure images are secure by scanning for and identifying vulnerabilities.</li> <li>Build &amp; Task Automation: Automate image builds and updates directly from source code repositories.</li> </ul> <p>Typical Use Cases:</p> <ul> <li>Storing Docker container images.</li> <li>Housing Helm charts and OCI artifacts.</li> <li>Integrating with CI/CD pipelines for continuous deployment.</li> </ul>"},{"location":"architecture/components/#azure-container-instance-aci","title":"Azure Container Instance (ACI)","text":"<p>Azure Container Instance (ACI) is a serverless container service that allows you to run containers without managing underlying virtual machines.</p> <p>Key Features of ACI:</p> <ul> <li>On-Demand Deployment: Quickly start, stop, and scale container instances as needed.</li> <li>No VM Management: Focus on developing applications without worrying about VM infrastructure.</li> <li>CPU/GPU Options: Choose from a variety of CPU and GPU configurations to match workload requirements.</li> <li>Pay as You Go: Only pay for the compute resources used while the container is running.</li> <li>Networking: Connect securely to other Azure resources using VNET integration.</li> </ul> <p>Typical Use Cases:</p> <ul> <li>Simple, stateless applications.</li> <li>Microservices orchestration.</li> <li>CI/CD jobs and Dev/Test environments.</li> <li>Batch processing and task automation.</li> </ul> <p>By integrating ACR and ACI with Docker in this workflow, the project benefits from streamlined image management, secure storage, and scalable, serverless container deployments. This setup ensures a robust, efficient, and flexible deployment pipeline for your API, leveraging the powerful capabilities of Microsoft Azure.</p>"},{"location":"architecture/components/#deployment-workflow","title":"Deployment Workflow","text":"<p>The deployment process for each component of the project typically follows these steps:</p> <ol> <li>Build a Docker Image: Begin by building the Docker image for the component.</li> <li>Push to Azure Container Registry: Push the Docker image to the project's Azure Container Registry (ACR).</li> <li>Deploy to Azure Resources: Deploy the image to the appropriate Azure resource, such as Azure Container Instances or Azure Web Apps.</li> </ol> <p>The diagram below illustrates this workflow:</p> <p></p>"},{"location":"architecture/docker/","title":"Docker","text":""},{"location":"architecture/docker/#docker-overview","title":"Docker Overview","text":"<p>Docker is an open-source platform that enables developers to automate the deployment of applications inside lightweight, portable containers. Containers bundle the application code with its dependencies and environment, ensuring consistency across different environments, such as development, testing, and production. By using Docker, we can create reproducible and isolated environments that simplify the deployment process and enhance scalability.</p>"},{"location":"architecture/docker/#api-example","title":"API Example","text":"<p>In our project, we have utilized Docker to containerize the API, making it easy to deploy and manage across various environments. Below is the Dockerfile used to create the container image for our API:</p> <pre><code># Use the official Python image from the Docker Hub\nFROM python:3.9-slim\n\n# Set the working directory\nWORKDIR /app\n\n# Copy the requirements file\nCOPY requirements.txt .\n\n# Install the required packages\nRUN pip install --no-cache-dir -r requirements.txt\n\n# Copy the rest of the application code\nCOPY . .\n\n# Expose the port the app runs on\nEXPOSE 80\n\n# Define the command to run the application\nCMD [\"uvicorn\", \"main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"80\"]\n</code></pre> <p>This Dockerfile begins by pulling the official Python 3.9-slim image from Docker Hub, providing a lightweight base for our API. The working directory is set to <code>/app</code>, where the application code will reside. The <code>requirements.txt</code> file, containing the necessary dependencies, is copied into the container, and the required packages are installed using <code>pip</code>.</p> <p>After the dependencies are installed, the rest of the application code is copied into the container. The <code>EXPOSE</code> instruction specifies that the container listens on port 80. Finally, the <code>CMD</code> instruction defines the command to start the API using Uvicorn, specifying that it should run on all available network interfaces (<code>0.0.0.0</code>) on port 80.</p> <p>This setup ensures that our API is encapsulated in a consistent environment, ready for deployment across different stages of development and production.</p>"},{"location":"architecture/docker/#docker-workflow","title":"Docker Workflow","text":"<p>To push our Docker image to Azure Container Registry (ACR), we follow these steps:</p> <p>Build the Docker Image</p> <p>First, we build the Docker image using the <code>docker build</code> command. This command reads the <code>Dockerfile</code> in the current directory and creates an image based on its instructions.</p> <pre><code>docker build -t &lt;image-name&gt;:&lt;tag&gt; .\n</code></pre> <ul> <li><code>&lt;image-name&gt;</code>: The name we want to assign to the Docker image.</li> <li><code>&lt;tag&gt;</code>: The version tag for the image (e.g., <code>v1.0.0</code>).</li> </ul> <p>Login to Azure Container Registry</p> <p>Authenticate Docker with our Azure Container Registry using the <code>az acr login</code> command.</p> <pre><code>az acr login --name &lt;acr-name&gt;\n</code></pre> <p>Alternatively, we can log in directly using Docker:</p> <pre><code>docker login &lt;acr-name&gt;.azurecr.io\n</code></pre> <ul> <li><code>&lt;acr-name&gt;</code>: This is the name of our Azure Container Registry.</li> <li>We are prompted to enter our username and password, which can be obtained from the Azure portal.</li> </ul> <p>Tag the Docker Image</p> <p>Tag the Docker image with the ACR repository URL to prepare it for pushing. We replace <code>&lt;acr-name&gt;</code>, <code>&lt;image-name&gt;</code>, and <code>&lt;tag&gt;</code> with our specific details.</p> <pre><code>docker tag &lt;image-name&gt;:&lt;tag&gt; &lt;acr-name&gt;.azurecr.io/&lt;image-name&gt;:&lt;tag&gt;\n</code></pre> <ul> <li><code>&lt;acr-name&gt;</code>: The name of our Azure Container Registry.</li> <li><code>&lt;image-name&gt;</code>: The name of our Docker image.</li> <li><code>&lt;tag&gt;</code>: The version tag for the image.</li> </ul> <p>Push the Docker Image to ACR</p> <p>Push the tagged Docker image to our Azure Container Registry using the <code>docker push</code> command.</p> <pre><code>docker push &lt;acr-name&gt;.azurecr.io/&lt;image-name&gt;:&lt;tag&gt;\n</code></pre> <ul> <li><code>&lt;acr-name&gt;</code>: The name of our Azure Container Registry.</li> <li><code>&lt;image-name&gt;</code>: The name of our Docker image.</li> <li><code>&lt;tag&gt;</code>: The version tag for the image.</li> </ul> <p>Verify the Image</p> <p>After pushing, we can verify that the image has been successfully uploaded to ACR by listing the images in the registry.</p> <pre><code>az acr repository list --name &lt;acr-name&gt; --output table\n</code></pre> <ul> <li><code>&lt;acr-name&gt;</code>: The name of our Azure Container Registry.</li> </ul> <p>This workflow ensures that our Docker image is built, tagged, and pushed to Azure Container Registry, making it available for deployment in our cloud environment.</p>"},{"location":"architecture/overview/","title":"Overview","text":""},{"location":"architecture/overview/#architecture-overview","title":"Architecture Overview","text":"<p>Our project is designed as a full-stack application, integrating a database, backend API, and frontend interface to deliver a comprehensive solution. The architecture is built to efficiently handle data processing, API requests, and user interactions, while leveraging Azure's cloud services to manage and scale our resources.</p>"},{"location":"architecture/overview/#key-components","title":"Key Components","text":"<p>Database </p> <p>The database serves as the foundation of our data management, storing and organizing the information needed by both the API and the frontend. We utilize a SQL-based relational database hosted on Azure, ensuring robust performance, security, and scalability.</p> <p>Backend API </p> <p>The API acts as the intermediary between the database and the frontend. It is built using FastAPI, a modern web framework for building APIs with Python. The API handles data processing, business logic, and communication with the database, providing endpoints for data retrieval, updates, and analytical functions. The API is containerized and deployed on Azure Container Instances for easy scaling and management.</p> <p>Frontend </p> <p>The frontend is the user-facing part of the application, developed using Vue.js. It provides an intuitive interface for users to interact with the data, visualize results, and perform various operations. The frontend communicates with the backend API to fetch and display data in real-time, ensuring a seamless user experience.</p> <p>Azure Integration </p> <p>Azure is central to our infrastructure management, providing services for hosting, scaling, and monitoring our application. We use Azure Resource Manager (ARM) to define and manage resources, including our database, container registry, and container instances. Terraform is employed to define our infrastructure as code (IaC), enabling consistent and repeatable deployments.</p> <p>This architecture ensures a modular, scalable, and maintainable system that can handle the demands of a dynamic and data-intensive application. By utilizing Azure's cloud capabilities, we are able to efficiently manage our resources, ensuring high availability and performance.</p>"},{"location":"architecture/terraform/","title":"Terraform","text":""},{"location":"architecture/terraform/#terraform-overview","title":"Terraform Overview","text":"<p>Terraform is an open-source Infrastructure as Code (IaC) tool that allows you to define and provision data center infrastructure using a high-level configuration language. With Terraform, you can manage resources across various cloud providers, such as Azure, AWS, and Google Cloud, in a consistent and repeatable manner. </p> <p>In our project, we have used Terraform to define and manage our infrastructure on Azure. This approach enables us to automate the deployment and management of resources, ensuring that our infrastructure is consistent, scalable, and easily reproducible.</p> <p>For example, below is the Terraform configuration file that outlines our API infrastructure setup:</p> <pre><code># Define provider\nprovider \"azurerm\" {\n  features {}\n}\n\n# Define resource group\nresource \"azurerm_resource_group\" \"masters_uct\" {\n  name     = \"Masters-UCT\"\n  location = \"UK South\"\n}\n\n# Define our container registry\nresource \"azurerm_container_registry\" \"acr\" {\n  name                = \"mastersuctacr\"\n  resource_group_name = azurerm_resource_group.masters_uct.name\n  location            = azurerm_resource_group.masters_uct.location\n  sku                 = \"Basic\"\n  admin_enabled       = true\n}\n\n# Define Container Instance for public API\nresource \"azurerm_container_group\" \"aci\" {\n  name                = \"mastersuctaci\"\n  resource_group_name = azurerm_resource_group.masters_uct.name\n  location            = azurerm_resource_group.masters_uct.location\n  os_type             = \"Linux\"\n\n  container {\n    name   = \"fsd-public-api\"\n    image  = \"${azurerm_container_registry.acr.login_server}/fsd_public_api:v1.0.0\"\n    cpu    = \"0.5\"\n    memory = \"1.5\"\n\n    ports {\n      port     = 80\n      protocol = \"TCP\"\n    }\n  }\n\n  ip_address_type = \"Public\"\n  dns_name_label = \"fsdpublicapi\"\n  image_registry_credential {\n    server   = azurerm_container_registry.acr.login_server\n    username = azurerm_container_registry.acr.admin_username\n    password = azurerm_container_registry.acr.admin_password\n  }\n}\n\noutput \"container_registry_login_server\" {\n  value = azurerm_container_registry.acr.login_server\n}\n\noutput \"container_instance_fqdn\" {\n  value = azurerm_container_group.aci.fqdn\n}\n</code></pre> <p>This Terraform configuration begins by specifying the Azure provider, ensuring that our deployment targets the correct cloud environment.</p> <ol> <li> <p>Resource Group: </p> <ul> <li>We define a resource group named <code>Masters-UCT</code>, located in the <code>UK South</code> region. This resource group acts as an organizational container for all our related resources.</li> </ul> </li> <li> <p>Azure Container Registry (ACR): </p> <ul> <li>Within this resource group, we create an Azure Container Registry (ACR) named <code>mastersuctacr</code>. This ACR allows us to store and manage Docker container images, with the admin feature enabled for easier access.</li> </ul> </li> <li> <p>Azure Container Instance (ACI):</p> <ul> <li>To deploy our public API, we utilize an Azure Container Instance (ACI) named <code>mastersuctaci</code>. This instance runs a Linux-based container, specifically the <code>fsd-public-api</code> image, which is pulled directly from our ACR. The container is configured with 0.5 CPUs and 1.5 GB of memory, with port 80 open for HTTP traffic.</li> <li>The ACI is assigned a public IP address and is accessible via a DNS name label <code>fsdpublicapi</code>, enabling users to interact with our API.</li> </ul> </li> <li> <p>Outputs:</p> <ul> <li>The configuration outputs the login server URL for the container registry and the fully qualified domain name (FQDN) for the container instance. These outputs provide essential endpoints for managing and accessing our deployed resources.</li> </ul> </li> </ol> <p>This Terraform setup ensures that our infrastructure is consistent, scalable, and easy to manage.</p>"},{"location":"architecture/terraform/#terraform-workflow-in-our-project","title":"Terraform Workflow in our Project","text":"<p>To apply changes defined in a Terraform configuration file, we generally follow a series of commands. Here's how they works:</p>"},{"location":"architecture/terraform/#terraform-commands","title":"Terraform Commands","text":"<ol> <li> <p><code>terraform init</code>:</p> <ul> <li>Purpose: This command initializes the Terraform working directory. It downloads the necessary provider plugins specified in our configuration file (e.g., <code>azurerm</code> for Azure) and sets up the backend for storing the state file (<code>terraform.tfstate</code>).</li> <li>When to Run: We run this command once after creating or cloning a Terraform configuration. It\u2019s also necessary after adding new providers or modules to the configuration.</li> </ul> </li> <li> <p><code>terraform plan</code>: </p> <ul> <li>Purpose: This command creates an execution plan, showing us what Terraform will do when we apply our changes. It compares the state file with our current configuration to determine what needs to be created, modified, or destroyed.</li> <li>When to Run: We use this command to review the changes before applying them. It\u2019s a good practice to run it every time we make changes to your configuration file.</li> </ul> </li> <li> <p><code>terraform apply</code>: </p> <ul> <li>Purpose: This command applies the changes required to reach the desired state of the configuration, as outlined by the plan. It prompts us to confirm the execution of the changes by typing \"yes\".</li> <li>When to Run: After reviewing the plan, we run this command to apply the changes to your infrastructure.</li> </ul> </li> <li> <p><code>terraform destroy</code>: </p> <ul> <li>Purpose: This command destroys all resources managed by our Terraform configuration. It is useful for cleaning up resources when they are no longer needed.</li> <li>When to Run: We use this command when we want to completely remove our infrastructure. It will ask for confirmation before proceeding.</li> </ul> </li> </ol>"},{"location":"architecture/terraform/#terraform-state-tfstate","title":"Terraform State (<code>tfstate</code>)","text":"<ul> <li> <p>What It Is: Terraform's state file (<code>terraform.tfstate</code>) is a JSON file that keeps track of the infrastructure resources managed by Terraform. It acts as a snapshot of the deployed infrastructure, storing information about the current state of resources, including IDs, attributes, and metadata.</p> </li> <li> <p>Purpose: The state file is essential for Terraform to understand what has been created, modified, or destroyed. When we run <code>terraform plan</code> or <code>terraform apply</code>, Terraform compares the current state in the <code>tfstate</code> file with the desired state defined in your configuration file to determine the necessary actions.</p> </li> <li> <p>Storage: The state file is stored locally by default, but it can (and should) be stored remotely in a secure backend (e.g., Azure Blob Storage, AWS S3) to facilitate collaboration and ensure consistency across multiple team members.</p> </li> </ul>"},{"location":"architecture/terraform/#terraform-diff","title":"Terraform Diff","text":"<ul> <li> <p>What It Is: The diff in Terraform refers to the difference between the current state of our infrastructure (as recorded in the <code>tfstate</code> file) and the desired state defined in our Terraform configuration.</p> </li> <li> <p>Purpose: The diff is generated when we run <code>terraform plan</code>. It shows a summary of what will change if we run <code>terraform apply</code>. This includes resources to be created, modified, or destroyed, marked with <code>+</code>, <code>~</code>, and <code>-</code> respectively in the output.</p> </li> <li> <p>Importance: Reviewing the diff before applying changes is critical to ensure that the planned modifications align with your expectations. It helps avoid unintentional changes to our infrastructure.</p> </li> </ul> <p>By running these Terraform commands and understanding the role of the <code>tfstate</code> file and the diff, we can effectively manage our infrastructure changes, ensuring they are applied safely and accurately.</p>"},{"location":"architecture/testing/","title":"Testing","text":""},{"location":"architecture/testing/#testing-overview","title":"Testing Overview","text":"<p>Our project places a strong emphasis on ensuring code quality, functionality, and reliability through comprehensive testing. We have developed an extensive test suite that covers various aspects of the application, including unit tests, integration tests, and API endpoint tests. These tests are crucial for validating the correctness of our analytical functions, ensuring seamless interaction with external APIs, and confirming that the core features of our application are functioning as expected.</p> <p>Our test suite is executed automatically as part of our Continuous Integration (CI) pipeline implemented in GitHub Actions. This setup allows us to catch potential issues early in the development process, maintain high code quality standards, and deliver a robust application to our users.</p>"},{"location":"architecture/testing/#ci-pipeline","title":"CI Pipeline","text":"<p>All commits and pull requests are automatically validated through our CI pipeline, which runs the entire test suite. Any failures trigger immediate notifications, ensuring that issues are addressed promptly. This automated process helps us maintain consistency and reliability throughout the development lifecycle.</p>"},{"location":"architecture/testing/#test-suite-details","title":"Test Suite Details","text":"<p>Our test suite is divided into two primary categories:</p>"},{"location":"architecture/testing/#1-analytical-function-tests","title":"1. Analytical Function Tests","text":"<p>Below are some example tests that focus on the core analytical functions that drive the data processing and analysis in our application. The functions are rigorously tested with a sample dataset to ensure they produce accurate and expected results. The key functions tested include:</p> <ul> <li> <p><code>daily_price_change</code>: Calculates the daily price change for each cryptocurrency. Test: We verify that the function correctly calculates the daily price change in percentage for the sample data.</p> </li> <li> <p><code>daily_price_range</code>: Determines the daily price range (high-low) for each cryptocurrency. Test: This test checks if the calculated price range matches the expected values for each day.</p> </li> <li> <p><code>daily_price_range_volatility</code>: Computes the volatility based on the daily price range. Test: We confirm that the function produces the correct volatility ratios, ensuring the reliability of the volatility analysis.</p> </li> <li> <p><code>daily_price_volatility</code>: Analyzes the overall volatility based on daily closing prices. Test: The test verifies the consistency of the volatility calculation across the dataset.</p> </li> <li> <p><code>moving_average</code>: Calculates the moving average of the closing prices over a specified window. Test: We check if the function accurately computes moving averages with various window sizes.</p> </li> <li> <p><code>find_peaks_and_valleys</code>: Identifies peaks and valleys in the price data. Test: This test ensures the function correctly identifies high and low points in the price trends.</p> </li> <li> <p><code>correlation_analysis</code>: Analyzes correlations between different numerical features in the dataset. Test: The correlation matrix generated by this function is tested for accuracy and correct dimensionality.</p> </li> </ul>"},{"location":"architecture/testing/#2-api-endpoint-tests","title":"2. API Endpoint Tests","text":"<p>Below are some of the tests that validate the functionality of our RESTful API endpoints, ensuring they return the expected responses and handle edge cases gracefully. The endpoints tested include:</p> <ul> <li> <p><code>/query_coin</code>: Tests the retrieval of coin data for specific cryptocurrencies. Test: We ensure that the endpoint returns the correct transaction state and the expected data structure.</p> </li> <li> <p><code>/daily_price_change</code>: Validates the daily price change calculations for multiple coins. Test: This test confirms that the endpoint processes the data correctly and returns the expected results.</p> </li> <li> <p><code>/daily_price_range</code>: Verifies the daily price range calculations across selected coins. Test: We check that the endpoint accurately computes and returns the daily price ranges.</p> </li> <li> <p><code>/moving_averages</code>: Assesses the moving average calculations provided by the endpoint. Test: This test checks the correct computation of moving averages for specified coins and time windows.</p> </li> <li> <p><code>/correlation_analysis</code>: Tests the correlation matrix generation for selected coins. Test: We ensure that the endpoint returns a properly formatted correlation matrix.</p> </li> <li> <p><code>/coin_reporting</code>: Tests the reporting functionality for a specific coin. Test: This endpoint is tested to confirm it returns the expected report data and transaction state.</p> </li> <li> <p><code>/coin_proportion</code>: Evaluates the distribution and proportion of selected coins. Test: This test verifies that the endpoint returns correct proportion data.</p> </li> <li> <p>User Authentication Endpoints (<code>/register</code>, <code>/login</code>): Tests: We use mock requests to test the user registration and login processes, ensuring they handle success and failure cases (e.g., existing user, invalid credentials) appropriately.</p> </li> </ul>"},{"location":"architecture/testing/#mocking-and-fixtures","title":"Mocking and Fixtures","text":"<p>To ensure isolated testing and prevent interference from external systems, we use fixtures and mocking techniques. Fixtures like <code>sample_data</code>, <code>new_user_data</code>, and <code>existing_user_data</code> provide consistent input across tests, while mocking enables us to simulate API responses for authentication-related tests.</p> <p>By implementing these tests, we maintain a high level of confidence in our application's reliability and accuracy, ensuring that our users can trust the insights and analyses provided by our platform.</p>"},{"location":"devops/github/","title":"Github","text":""},{"location":"devops/github/#overview","title":"Overview","text":"<p>Git is a distributed version control system that allows multiple developers to work on a project simultaneously without overwriting each other's changes. It keeps track of every modification to the project's files, enabling you to revert to previous versions if necessary. Git is essential for collaborative software development as it provides a structured way to manage and merge contributions from different team members, ensuring the project's integrity.</p> <p>GitHub is an online platform that hosts Git repositories, offering a web-based interface for managing Git projects. It facilitates collaboration by allowing developers to share their repositories, review code, track issues, and deploy applications. GitHub also provides tools for continuous integration and deployment, making it an invaluable resource for modern software development. </p> <p>Using Git and GitHub in coding projects ensures a well-organized workflow, where changes are documented, collaboration is streamlined, and the project history is preserved, which is crucial for both development and maintenance.</p>"},{"location":"devops/github/#git-management","title":"Git Management","text":"<p>Effective Git management involves several key practices to maintain a clean and efficient workflow. These include:</p> <ul> <li> <p>Branching Strategy: Implementing a branching strategy (such as GitFlow or Feature Branching) ensures that development, testing, and deployment processes are well-organized. For example, keeping the main branch stable while using feature branches for new developments helps maintain the project's integrity.</p> </li> <li> <p>Commit Practices: Writing clear and concise commit messages, and committing changes frequently with meaningful descriptions, makes it easier to understand the project's history and roll back changes if needed.</p> </li> <li> <p>Merge and Pull Requests: Regularly merging branches and using pull requests to review and discuss changes before integrating them into the main branch is essential for maintaining code quality and preventing conflicts.</p> </li> </ul> <p>By adhering to these Git management practices, teams can ensure that their project remains organized, scalable, and easy to maintain throughout its lifecycle.</p>"},{"location":"devops/github/#fsd-general-git-management","title":"FSD General Git Management","text":"<p>The repository exists of multiple branches that have been created for testing, bug fixes, and individual development. This allows the team to develop in parallel. The repository also has a main branch and a main staging branch. Both of these branches are considered \"production\" ready branches. The staging branch is a buffer for the commits before they are moved into the main branch.</p> <p>The general git workflow for the backend system can be analyzed in the git workflow diagram but on a high level the workflow is as follows:</p> <ul> <li>2 protected branches ( main, main_staging )</li> <li>These branches have branch rules that restrict commits being pushed without a PR.</li> <li>When creating a PR into the protected branch, the commit will run  through a CI pipeline and automatically formats the code, suggests  changes and checks for errors through static type checking and pytests</li> <li>If an error occurs during the CI pipeline, the PR is rejected until the issues are fixed.</li> <li>Developers can commit normally to any other branches</li> </ul>"},{"location":"devops/github/#git-workflow-visualized","title":"Git Workflow Visualized","text":"<p>The image above illustrates the general workflow for committing changes to the repository. Starting from the green circle, the process unfolds as follows:</p> <ul> <li>A developer makes a commit.</li> <li>Is the branch a protected branch?<ul> <li>If not protected, the developer can continue committing directly to the branch.</li> <li>If protected, the commit cannot be pushed directly; a pull request (PR) is required.</li> </ul> </li> <li>The developer must create a PR if the branch is protected.</li> <li>Once the PR is created, it is automatically run through a Continuous Integration (CI) pipeline that includes code tests and linting checks.<ul> <li>If the PR fails the CI checks, the developer is notified of the failure and must address the issues, starting the process again from the green circle.</li> </ul> </li> <li>If the PR passes the CI checks, it proceeds to the review stage, where it must be approved by at least two developers.<ul> <li>If any reviewer rejects the PR, the developer must resolve the issues and restart the process from the green circle.</li> </ul> </li> <li>If all reviewers approve the PR, the code is merged into the protected branch.</li> </ul>"},{"location":"devops/github/#ci-pipeline","title":"CI Pipeline","text":"<p>The Continuous Integration (CI) pipeline is a crucial part of our development process, ensuring that code changes meet our project's quality standards before they are merged into the main branch. The CI pipeline is automatically triggered when a developer pushes code to the <code>main</code> branch or creates a pull request targeting the <code>main</code> branch. The pipeline consists of several stages:</p> <ul> <li> <p>Setting up the Environment: The pipeline runs on the latest version of Ubuntu and uses Python 3.9.13. It checks out the repository code and installs the necessary dependencies, including tools like Flake8, Pytest, Black, and MyPy.</p> </li> <li> <p>Code Linting with Flake8: This step checks the code for syntax errors and enforces Python coding standards. If any issues are found, the pipeline fails, prompting the developer to fix them.</p> </li> <li> <p>Code Formatting with Black: The Black tool checks that the code adheres to our project's formatting guidelines. If the code isn't properly formatted, the pipeline fails, and the developer must reformat their code.</p> </li> <li> <p>Type Checking with MyPy: This step ensures that the code is type-safe by running MyPy, a static type checker. This helps catch potential bugs that could arise from type mismatches.</p> </li> <li> <p>Testing with Pytest: Finally, the pipeline runs our test suite using Pytest. All tests must pass for the pipeline to succeed, ensuring that the code is functional and that existing features haven't been broken by the new changes.</p> </li> </ul> <p>If any stage of the CI pipeline fails, the pull request is rejected, and the developer must resolve the issues before resubmitting. This automated process helps maintain high code quality and prevents problematic code from being merged into the main branch.</p> <p>Below is our CI yaml that initializes our pipeline:</p> <pre><code>name: Python application\n\non:\npush:\nbranches: [ \"main\" ]\npull_request:\nbranches: [ \"main\" ]\n\npermission:\ncontents: read\n\njobs:\nbuild:\n\nruns-on: ubuntu-latest\n\nsteps:\n- uses: actions/checkout@v4\n- name: Set Up Python 3.9.13\n\nuses: actions/setup-python@V3\nwith:\npython-version: \"3.9.13\"\n- name: Install dependencies\nrun: |\npython -m pip install --upgrade pip\npip install flake8 pytest black mypy\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n- name: Lint with flake8\nrun: |\n# stop the build if there are Python syntax errors or undefined names\nflake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\n\n- name: Check code formatting with Black\nrun: |\nblack --check .\n- name: Type check with MyPy\nrun: |\nmypy .\n- name: Test with pytest\nrun: |\npytest -v\n</code></pre>"},{"location":"devops/overview/","title":"Overview","text":""},{"location":"devops/overview/#devops-overview","title":"DevOps Overview","text":"<p>The DevOps strategy for our crypto financial dashboard project is designed to ensure continuous integration while maintaining high reliability and scalability. By leveraging modern DevOps practices and cloud-based tools, we aim to automate the entire development lifecycle\u2014from code commit to production deployment\u2014thus enabling rapid iteration and consistent delivery of new features and updates.</p>"},{"location":"devops/overview/#key-components-of-the-devops-pipeline","title":"Key Components of the DevOps Pipeline","text":"<p>Version Control and Collaboration</p> <ul> <li>All code is managed in a Git repository hosted on GitHub. GitHub serves as the central hub for version control, collaboration, and code review, ensuring that all changes are tracked and managed effectively - to read more on the projects git workflow you can see our git workflow page.</li> <li>Branching Strategy: We use a feature-branch workflow, where new features or bug fixes are developed in isolated branches and merged into the main branch only after passing code reviews and automated tests.</li> </ul> <p>Continuous Integration (CI)</p> <ul> <li>GitHub Actions is utilized to automate the CI process. Every code push triggers automated builds and tests, ensuring that the codebase remains stable and that new changes do not introduce regressions.</li> <li>Automated Testing: The CI pipeline includes unit tests, integration tests, and linting checks. These tests run automatically, providing immediate feedback to developers on the quality and correctness of their code.</li> </ul> <p>Containerization</p> <ul> <li>The application is containerized using Docker. Docker containers ensure that the application runs consistently across different environments, eliminating issues related to environment configuration and dependency management.</li> </ul> <p>Deployment to Azure</p> <ul> <li>The project is deployed to various Azure resources based on the component's requirements:<ul> <li>Azure Container Instances (ACI): For running stateless containers with low overhead.</li> <li>Azure Web App for Containers: For deploying web applications with integrated scaling and monitoring.</li> </ul> </li> </ul> <p>Collaboration and Documentation</p> <ul> <li>Documentation is generated and updated using tools like MkDocs with the Material theme. This ensures that all documentation is up-to-date and accessible to the team.</li> </ul>"},{"location":"devops/sprints/","title":"Sprints","text":""},{"location":"devops/sprints/#sprint-workflow","title":"Sprint Workflow","text":"<p>Our project follows an Agile development methodology, with work organized into iterative sprints. Each sprint is a time-boxed period, typically lasting two weeks, during which specific tasks and goals are completed. This approach allows for continuous delivery of incremental improvements and ensures that we can adapt to changes and feedback quickly.</p>"},{"location":"devops/sprints/#sprint-planning","title":"Sprint Planning","text":"<p>Defining Sprint Goals</p> <ul> <li>At the beginning of each sprint, the team holds a sprint planning meeting to define the objectives for the upcoming sprint. These objectives are typically derived from the product backlog, which is a prioritized list of features, enhancements, and bug fixes.</li> <li>The team discusses and agrees on the most critical items to focus on, ensuring that the sprint goal aligns with the overall project objectives.</li> </ul> <p>Task Breakdown and Estimation</p> <ul> <li>The selected sprint tasks are broken down into smaller, manageable units of work, often referred to as user stories or tasks. Each task is estimated in terms of effort, usually in story points or hours.</li> <li>The team collaboratively estimates the complexity and effort required for each task, considering factors like dependencies, risks, and potential challenges.</li> </ul> <p>Assigning Tasks</p> <ul> <li>Once tasks are defined and estimated, they are assigned to team members based on their expertise, availability, and workload. Task assignments are flexible and can be adjusted as the sprint progresses.</li> </ul>"},{"location":"devops/sprints/#sprint-execution","title":"Sprint Execution","text":"<p>Daily Standups</p> <ul> <li>Throughout the sprint, the team holds daily standup meetings, also known as daily scrums. These brief meetings allow each team member to share what they accomplished the previous day, what they plan to work on today, and any blockers or challenges they are facing.</li> <li>Daily standups foster communication, transparency, and collaboration among team members, ensuring that everyone is aligned and aware of the sprint's progress.</li> </ul> <p>Task Progress and Updates</p> <ul> <li>As tasks are worked on, team members update the status in the project management tool (Azure Boards). This helps track progress, identify bottlenecks, and ensure that the sprint stays on track.</li> <li>Regular check-ins and communication are encouraged to address any issues promptly and to provide support where needed.</li> </ul>"},{"location":"devops/sprints/#sprint-review-and-retrospective","title":"Sprint Review and Retrospective","text":"<p>Sprint Review</p> <ul> <li>At the end of each sprint, the team conducts a sprint review meeting. During this meeting, completed work is demonstrated to stakeholders, and feedback is gathered. This review helps ensure that the deliverables meet the expected quality and functionality.</li> <li>Stakeholders provide input, and any new requirements or changes are discussed and added to the product backlog if necessary.</li> </ul> <p>Sprint Retrospective</p> <ul> <li>Following the sprint review, the team holds a sprint retrospective meeting. This meeting is an opportunity to reflect on the sprint, discussing what went well, what could have been improved, and what actions can be taken to enhance future sprints.</li> <li>The retrospective is a key aspect of continuous improvement, allowing the team to learn from each sprint and make adjustments to their process, tools, or communication strategies.</li> </ul>"},{"location":"devops/sprints/#benefits-of-sprints","title":"Benefits of Sprints","text":"<ul> <li>Flexibility: Sprints allow the team to adapt to changes quickly, incorporating new requirements or feedback without disrupting the overall project timeline.</li> <li>Focus: By concentrating on a specific set of tasks each sprint, the team can deliver high-quality work and maintain a steady pace of progress.</li> <li>Continuous Feedback: Regular reviews and retrospectives ensure that the team receives continuous feedback, enabling them to make iterative improvements and deliver a product that meets stakeholder expectations.</li> </ul>"},{"location":"devops/sprints/#project-sprints","title":"Project Sprints","text":"<p>This section provides an overview of the project\u2019s sprints and details the tasks accomplished during each sprint. The accompanying images showcase all the tasks included in the sprint backlog.</p>"},{"location":"devops/sprints/#sprint-1-may-6-may-12","title":"Sprint 1 (May 6 - May 12)","text":""},{"location":"devops/sprints/#sprint-2-may-13-may-26","title":"Sprint 2 (May 13 - May 26)","text":""},{"location":"devops/sprints/#sprint-3-may-27-june-7","title":"Sprint 3 (May 27 - June 7)","text":""},{"location":"devops/sprints/#sprint-4-june-10-june-21","title":"Sprint 4 (June 10 - June 21)","text":""},{"location":"devops/sprints/#sprint-5-june-24-july-5","title":"Sprint 5 (June 24 - July 5)","text":""},{"location":"devops/sprints/#sprint-6-july-6-july-19","title":"Sprint 6 (July 6 - July 19)","text":""},{"location":"devops/userstories/","title":"User Stories","text":""},{"location":"devops/userstories/#view-latest-cryptocurrency-data","title":"View Latest Cryptocurrency Data","text":"<p>As a financial analyst, I want to view cryptocurrency data, including prices, volumes, and market caps, so that I can make informed decisions about market trends.</p>"},{"location":"devops/userstories/#sign-in-to-dashboard","title":"Sign In to Dashboard","text":"<p>As a user, I want to sign in to the financial dashboard so that I can access personalized insights and data relevant to my portfolio.</p>"},{"location":"devops/userstories/#sign-up-for-dashboard","title":"Sign Up for Dashboard","text":"<p>As a new user, I want to sign up for the financial dashboard so that I can start tracking and analyzing cryptocurrency data.</p>"},{"location":"devops/userstories/#view-summary-of-top-5-cryptocurrencies","title":"View Summary of Top 5 Cryptocurrencies","text":"<p>As a user, I want to see a summary of the top 5 cryptocurrencies on the dashboard so that I can quickly assess the most significant market movers.</p>"},{"location":"devops/userstories/#generate-and-view-daily-price-change-graphs","title":"Generate and View Daily Price Change Graphs","text":"<p>As a financial data scientist, I want to generate and view daily price change graphs and moving averages so that I can analyze short-term trends and volatility.</p>"},{"location":"devops/userstories/#see-trending-cryptocurrencies-and-largest-gainers","title":"See Trending Cryptocurrencies and Largest Gainers","text":"<p>As a user, I want to see trending cryptocurrencies and largest gainers in the market so that I can identify potential investment opportunities.</p>"},{"location":"devops/userstories/#view-detailed-market-data","title":"View Detailed Market Data","text":"<p>As a user, I want to view detailed market data, including trading volume and market cap, so that I can understand the market dynamics of different cryptocurrencies.</p>"},{"location":"devops/userstories/#sign-out-of-dashboard","title":"Sign Out of Dashboard","text":"<p>As a user, I want to sign out of the dashboard to ensure that my account remains secure when I am not using the platform.</p>"},{"location":"devops/userstories/#navigate-between-dashboard-sections","title":"Navigate Between Dashboard Sections","text":"<p>As a user, I want to navigate between different sections of the dashboard, such as data information, machine learning, and settings, so that I can easily access the tools and data I need.</p>"},{"location":"frontend/components/","title":"Components","text":""},{"location":"frontend/components/#dashboard-component-financialdashboardvue","title":"Dashboard Component (FinancialDashboard.vue)","text":"<p>This component is the main dashboard of the application where users can view and interact with different financial graphs based on selected cryptocurrencies and date ranges.</p> <ul> <li> <p>Template:</p> <ul> <li>Contains a header with controls for selecting coins and date ranges.</li> <li>Displays a series of graphs based on the user's selections, each within its own el-card.</li> <li>The el-select component allows users to choose multiple coins, and the el-date-picker is used for selecting date ranges.</li> <li>The fetchAllGraphsData button triggers data retrieval, while the clearSelections button resets all selections.</li> <li>Graphs are displayed using the PlotlyChart component.</li> </ul> </li> <li> <p>Script:</p> <ul> <li>Imports and uses Axios for API calls to fetch coin options and graph data.</li> <li>Maintains the state for selected coins, date range, available coin options, and graph data.</li> <li>The fetchCoinOptions method retrieves available cryptocurrency options from the backend.</li> <li>The fetchAllGraphsData method is responsible for fetching and updating the data for all graphs, based on the user's selections.</li> <li>Contains utility methods such as formatDate for date formatting and getGraphHelpText to provide descriptions for each graph.</li> </ul> </li> <li> <p>Style:</p> <ul> <li>Styles the dashboard container and components to be visually organized and responsive.</li> <li>The header controls are aligned for easy user interaction.</li> <li>Each chart is encapsulated in a styled el-card with hover effects and appropriate padding.</li> </ul> </li> </ul>"},{"location":"frontend/components/#plotly-chart-component-plotlychartvue","title":"Plotly Chart Component (PlotlyChart.vue)","text":"<p>This component is used to render Plotly charts within the application.</p> <ul> <li> <p>Template:</p> <ul> <li>A single div element with a reference (ref) for rendering the Plotly chart.</li> </ul> </li> <li> <p>Script:</p> <ul> <li>Imports Plotly for chart rendering.</li> <li>Accepts data, layout, and options as props to configure the chart.</li> <li>The drawChart method is called on component mount and whenever data or layout changes, ensuring the chart is updated dynamically.</li> <li>Uses Plotly.newPlot to render the chart within the referenced div.</li> </ul> </li> <li> <p>Style:</p> <ul> <li>Scoped styles can be added if needed, though none are specified in this component.</li> </ul> </li> </ul>"},{"location":"frontend/components/#sign-in-component-signinvue","title":"Sign-In Component (SignIn.vue)","text":"<p>This component provides the user interface for signing into the application.</p> <ul> <li> <p>Template:</p> <ul> <li>Contains an el-form with fields for username and password.</li> <li>Includes a \"Sign In\" button that triggers the login process.</li> <li>Displays an error alert if there is a problem with the login process.</li> </ul> </li> <li> <p>Script:</p> <ul> <li>Uses Axios to send login credentials to the backend API.</li> <li>On successful login, the username is stored in localStorage, and the user is redirected to the FinancialDashboard.</li> <li>Handles and displays error messages in case of a failed login attempt.</li> </ul> </li> <li> <p>Style:</p> <ul> <li>Styles the form row to center it vertically on the page.</li> <li>Adds padding and margin to the card to ensure the form is well-structured and visually appealing.</li> </ul> </li> </ul>"},{"location":"frontend/components/#sign-up-component-signupvue","title":"Sign-Up Component (SignUp.vue)","text":"<p>This component provides the user interface for new users to create an account.</p> <ul> <li> <p>Template:</p> <ul> <li>Similar to the Sign-In component, it contains an el-form with fields for email, username, and password.</li> <li>Includes a \"Sign Up\" button that triggers the registration process.</li> <li>Displays an error alert if there is a problem during registration.</li> </ul> </li> <li> <p>Script:</p> <ul> <li>Sends registration data to the backend API using Axios.</li> <li>On successful registration, the username is stored in localStorage, and the user is redirected to the FinancialDashboard.</li> <li>Handles errors by displaying appropriate messages to the user.</li> </ul> </li> <li> <p>Style:</p> <ul> <li>Similar styling to the Sign-In component, ensuring consistency across the authentication pages.</li> </ul> </li> </ul>"},{"location":"frontend/overview/","title":"Overview","text":""},{"location":"frontend/overview/#overview","title":"Overview","text":"<p>In this documentation, we will cover the front-end implementation of a financial dashboard application, built using Vue.js as the primary framework and Element UI as the UI component library. We will provide an overview of these technologies, their benefits, and how they are utilized in the application. Following that, we will delve into the specific components of the application, detailing their structure, functionality, and styling.</p>"},{"location":"frontend/overview/#vuejs-framework","title":"Vue.js Framework","text":"<p>Vue.js is a progressive JavaScript framework that is widely used for building user interfaces, particularly single-page applications (SPAs). Vue\u2019s core library focuses exclusively on the view layer, making it easy to integrate with other libraries or existing projects. Vue.js offers several key features that make it a popular choice for developers:</p> <ul> <li>Reactive Data Binding: Vue.js automatically updates the UI when the underlying data changes, making it easier to manage and synchronize application state.</li> <li>Component-Based Architecture: Vue encourages a modular approach to development, where the UI is divided into reusable, self-contained components.</li> <li>Virtual DOM: Vue.js uses a virtual DOM for efficient rendering, updating only the parts of the DOM that have changed.</li> <li>Ecosystem and Tooling: Vue has a rich ecosystem that includes Vue Router for routing, Vuex for state management, and integrations with modern JavaScript tools.</li> </ul>"},{"location":"frontend/overview/#benefits-of-using-vuejs","title":"Benefits of Using Vue.js","text":"<ul> <li>Ease of Learning: With its simplicity and clear documentation, Vue.js is accessible to both beginners and experienced developers.</li> <li>Flexibility: Vue can be used for small projects or scaled up for large, complex applications, adapting to the needs of the development team.</li> <li>Strong Community Support: A large, active community provides a wealth of resources, including plugins, libraries, and tools.</li> </ul>"},{"location":"frontend/overview/#element-ui-based-on-bootstrap","title":"Element UI (Based on Bootstrap)","text":"<p>Element UI is a Vue.js-based component library that provides a wide range of pre-built, customizable UI components. It draws inspiration from Bootstrap, a popular front-end framework, ensuring that the components are both functional and visually appealing.</p>"},{"location":"frontend/overview/#key-features-of-element-ui","title":"Key Features of Element UI","text":"<ul> <li>Rich Set of Components: Element UI includes components like buttons, forms, tables, and dialogs, which are fully responsive and customizable.</li> <li>Customizable Themes: Developers can easily customize the appearance of components to match their brand\u2019s design language.</li> <li>Ease of Integration: As a library designed for Vue.js, Element UI integrates seamlessly into Vue projects, making it easy to use.</li> <li>Internationalization Support: Element UI supports multiple languages, allowing developers to build multilingual applications.</li> </ul>"},{"location":"frontend/overview/#benefits-of-using-element-ui","title":"Benefits of Using Element UI","text":"<ul> <li>Rapid Development: By providing pre-built components, Element UI speeds up the development process, allowing developers to focus on application logic.</li> <li>Consistency: Using a standardized set of components ensures that the UI is consistent across different sections of the application.</li> <li>Responsive Design: Element UI components are designed to work well on various devices, from desktops to mobile phones.</li> <li>Extensive Documentation: Detailed documentation and examples for each component make it easy for developers to implement and customize them.</li> </ul>"},{"location":"frontend/screen_mockups/","title":"Mockups","text":""},{"location":"frontend/screen_mockups/#screen-mockups","title":"Screen mockups","text":""},{"location":"frontend/screen_mockups/#landing-page","title":"Landing Page","text":""},{"location":"frontend/screen_mockups/#sign-inup","title":"Sign in/up","text":""},{"location":"frontend/screen_mockups/#home-page","title":"Home page","text":""},{"location":"frontend/screen_mockups/#financial-dashboard-page","title":"Financial Dashboard Page","text":""},{"location":"frontend/screen_mockups/#data-information-page","title":"Data Information Page","text":""},{"location":"frontend/screen_mockups/#machine-learning-page","title":"Machine Learning Page","text":""},{"location":"frontend/screen_mockups/#user-settings-page","title":"User Settings Page","text":""},{"location":"microservices/api/","title":"Public API","text":""},{"location":"microservices/api/#fsd-public-api","title":"FSD Public API","text":"<p>In addition to our primary project, which delivers a robust analytical engine and an interactive dashboard for in-depth market analysis, the team has developed a separate public API. This decision was made to address specific needs and opportunities for broader access and usability. Here\u2019s why the public API is a crucial component:</p>"},{"location":"microservices/api/#enhanced-accessibility","title":"Enhanced Accessibility","text":"<p>The public API offers a streamlined way for developers and stakeholders to integrate our data analysis functions into their own applications and tools. By providing programmatic access, we extend our reach and facilitate greater interaction with users, allowing them to leverage our capabilities in various contexts.</p>"},{"location":"microservices/api/#improved-financial-literacy","title":"Improved Financial Literacy","text":"<p>By making financial analytics and visualizations available through an API, we aim to promote financial literacy. Users can create custom applications or dashboards tailored to their specific needs, helping them understand market trends and make well-informed decisions.</p>"},{"location":"microservices/api/#encouraging-innovation","title":"Encouraging Innovation","text":"<p>The API fosters innovation by enabling the creation of new tools and applications in the financial technology space. This separation allows external developers to experiment and build on our work, leading to novel use cases and solutions that enhance the ecosystem.</p>"},{"location":"microservices/api/#focused-development","title":"Focused Development","text":"<p>Developing the API separately ensures that we can tailor it specifically for integration and accessibility without compromising the core functionality and performance of our main analytical tools. This approach helps maintain high quality and reliability in both the API and the primary application.</p>"},{"location":"microservices/ingestion/","title":"Kaggle Ingestion","text":""},{"location":"microservices/ingestion/#benefits-of-the-kaggle-data-ingestion-tool","title":"Benefits of the Kaggle Data Ingestion Tool","text":"<p>The Kaggle Data Ingestion Tool is designed to simplify the process of downloading and managing Kaggle datasets. Here\u2019s how it adds value to your workflow:</p> <ol> <li> <p>Streamlined Data Acquisition: The tool automates the process of downloading datasets from Kaggle, reducing manual effort and ensuring consistency. Users only need to provide the dataset name, and the tool handles the rest, including optional unzipping of files.</p> </li> <li> <p>Flexible Configuration: Users can specify whether they want the downloaded files to be unzipped and choose the download path. This flexibility accommodates different project needs and data management preferences.</p> </li> <li> <p>Error Handling and Validation: The tool includes robust error handling and validation mechanisms. It checks for the correct format of the dataset name and provides clear error messages, helping users avoid common mistakes and ensuring data integrity.</p> </li> <li> <p>Enhanced Usability: By integrating command-line arguments for customization, users can easily tailor the data download process to their needs without modifying the code. This feature enhances the tool\u2019s usability and adaptability in various environments.</p> </li> <li> <p>Seamless Integration: The tool can be incorporated into other systems or microservices to automate data ingestion workflows. It supports integration with existing data pipelines and applications, providing a reliable solution for managing Kaggle datasets.</p> </li> </ol>"},{"location":"microservices/ingestion/#using-the-tool","title":"Using the Tool","text":""},{"location":"microservices/ingestion/#command-line-interface-cli","title":"Command-Line Interface (CLI)","text":"<p>To use the Kaggle Data Ingestion Tool via the command line, follow these steps:</p> <ol> <li> <p>Basic Usage:    <pre><code>python -m src.ingestion.ingestion [dataset_name]\n</code></pre>    Replace <code>[dataset_name]</code> with the name of the dataset you want to download (e.g., <code>sudalairajkumar/cryptocurrencypricehistory</code>).</p> </li> <li> <p>Unzipping the Files:    <pre><code>python -m src.ingestion.ingestion [dataset_name] --unzip\n</code></pre>    Add <code>--unzip</code> to automatically unzip the downloaded files.</p> </li> <li> <p>Custom Download Path:    <pre><code>python -m src.ingestion.ingestion [dataset_name] --path [path_to_download]\n</code></pre>    Replace <code>[path_to_download]</code> with the desired download location.</p> </li> <li> <p>Complete Example:    <pre><code>python -m src.ingestion.ingestion sudalairajkumar/cryptocurrencypricehistory --no-unzip --path ./data\n</code></pre>    This command will download the dataset without unzipping the files and save them in the <code>./data</code> directory.</p> </li> </ol>"},{"location":"microservices/overview/","title":"Overview","text":""},{"location":"microservices/overview/#microservices-overview","title":"Microservices Overview","text":"<p>Our system is designed with a modular architecture to enhance flexibility, scalability, and usability. Two key components of this architecture are the Public API and the Kaggle Data Ingestion Tool. These microservices are developed to streamline data management, improve accessibility, and integrate seamlessly with other applications.</p>"},{"location":"microservices/overview/#public-api","title":"Public API","text":"<p>Purpose</p> <p>The Public API is crafted to provide programmatic access to our financial analytics and visualization tools. It extends the capabilities of our primary project, which focuses on delivering a robust analytical engine and an interactive dashboard for in-depth market analysis. The API serves as a bridge for developers and stakeholders, allowing them to integrate our analytics into their own applications and tools.</p> <p>Key Benefits</p> <ul> <li>Enhanced Accessibility: Provides a streamlined way for users to access and integrate financial data analysis into their applications, promoting wider interaction and utilization.</li> <li>Improved Financial Literacy: Enables users to build custom applications or dashboards, fostering a deeper understanding of market trends and aiding in informed decision-making.</li> <li>Encouraging Innovation: Supports the creation of new tools and applications by allowing external developers to build upon our work, leading to innovative solutions and novel use cases.</li> <li>Focused Development: Separates the API development from core analytical tools to ensure high quality and reliability in both components.</li> </ul> <p>Usage</p> <p>To interact with the Public API, users can send HTTP requests to the defined endpoints. The API supports various operations, including data retrieval and analysis, making it a versatile tool for integrating financial insights into external systems.</p>"},{"location":"microservices/overview/#kaggle-data-ingestion-tool","title":"Kaggle Data Ingestion Tool","text":"<p>Purpose</p> <p>The Kaggle Data Ingestion Tool is designed to simplify the process of downloading and managing Kaggle datasets. It automates the data acquisition process, ensuring that users can efficiently obtain and handle data for analysis without manual intervention.</p> <p>Key Benefits</p> <ul> <li>Streamlined Data Acquisition: Automates dataset downloads from Kaggle, minimizing manual effort and ensuring consistent data retrieval.</li> <li>Flexible Configuration: Allows users to specify whether files should be unzipped and where they should be saved, accommodating various project needs and preferences.</li> <li>Error Handling and Validation: Includes robust mechanisms for validating dataset names and handling errors, preventing common mistakes and ensuring data integrity.</li> <li>Enhanced Usability: Supports command-line arguments for customization, making it easy to tailor the data download process to specific requirements.</li> <li>Seamless Integration: Can be incorporated into data pipelines and applications, providing a reliable solution for managing Kaggle datasets.</li> </ul> <p>Usage</p> <p>The Kaggle Data Ingestion Tool can be used both as a command-line interface (CLI) and an API endpoint. </p> <ul> <li>CLI Usage Users can download datasets by running commands in the terminal, with options to unzip files and specify download paths.</li> <li>API Integration The tool can be integrated into FastAPI applications, allowing users to request dataset downloads through API endpoints. This integration supports automation and enhances workflow efficiency.</li> </ul>"},{"location":"microservices/overview/#conclusion","title":"Conclusion","text":"<p>By leveraging these microservices, users benefit from a streamlined data management experience and enhanced accessibility to analytical tools. The Public API and Kaggle Data Ingestion Tool are integral components of our system, designed to support diverse data needs and foster innovation in financial data analysis.</p>"},{"location":"ml/functionality/","title":"Functionality","text":""},{"location":"ml/functionality/#key-modules-and-functions","title":"Key Modules and Functions","text":""},{"location":"ml/functionality/#1-feature-engineering","title":"1. Feature Engineering","text":"<ul> <li>add_date_features(df: pd.DataFrame) -&gt; pd.DataFrame: Enhances the dataset by adding date-related features such as the day of the week, month, year, and a flag for weekends. This enables the model to capture time-based patterns.</li> <li> <p>add_technical_features(df: pd.DataFrame) -&gt; pd.DataFrame: Integrates key technical indicators into the dataset, including Simple Moving Averages (SMA), Exponential Moving Averages (EMA), Bollinger Bands, MACD, RSI, and others. These indicators are crucial for identifying market trends and volatility.</p> </li> <li> <p>Technical Indicator Functions:</p> <ul> <li>calculate_sma: Computes the Simple Moving Average, which smooths out price data.</li> <li>calculate_ema: Calculates the Exponential Moving Average, giving more weight to recent data.</li> <li>calculate_rsi: Determines the Relative Strength Index, a momentum oscillator that measures the speed and change of price movements.</li> <li>calculate_macd: Computes the Moving Average Convergence Divergence, highlighting changes in the strength, direction, momentum, and duration of a trend.</li> <li>calculate_bollinger_bands: Establishes upper and lower bands around a SMA, indicating volatility.</li> </ul> </li> </ul>"},{"location":"ml/functionality/#2-data-preprocessing","title":"2. Data Preprocessing","text":"<ul> <li>preprocess_data(df: pd.DataFrame) -&gt; pd.DataFrame: Combines date features, technical indicators, and one-hot encoding of categorical features. Missing data is handled via mean imputation, ensuring that the dataset is clean and ready for modeling.</li> </ul>"},{"location":"ml/functionality/#3-model-training-and-prediction","title":"3. Model Training and Prediction","text":"<ul> <li>run_regressor(df: pd.DataFrame, alpha: float = 0.1) -&gt; None: Trains a Ridge Regression model using the processed data, saves the model, and generates predictions for selected cryptocurrencies like Bitcoin and Cardano. The model\u2019s top features are extracted and used for prediction, which is then visualized.</li> <li>run_gbm(df: pd.DataFrame) -&gt; None: Trains an XGBoost model, saves it, and generates predictions. The XGBoost model is particularly useful for handling large datasets with many features, offering high accuracy and speed.</li> <li>load_regression_model(file_path: str, df: pd.DataFrame, coin_names: List[str], start_date: str, end_date: str) -&gt; go.Figure: Loads a pre-trained Ridge Regression model and generates predictions for specific coins within a defined date range. The results are plotted, providing a visual comparison between actual and predicted prices.</li> </ul>"},{"location":"ml/functionality/#4-time-series-forecasting","title":"4. Time Series Forecasting","text":"<ul> <li>forecast_features_for_coin(coin_df: pd.DataFrame, coin_name: str, forecast_years: int, order=(2, 1, 2)) -&gt; pd.DataFrame: Uses the ARIMA model to forecast future values for key financial metrics (e.g., High, Low, Close, Volume) over a specified number of years. ARIMA is a powerful tool for time-series forecasting, handling trends and seasonality effectively.</li> <li>run_forecasts(df: pd.DataFrame) -&gt; pd.DataFrame: Automates the forecasting process for all cryptocurrencies in the dataset, consolidating the results into a single DataFrame and saving them for further analysis.</li> </ul>"},{"location":"ml/functionality/#5-utility-functions","title":"5. Utility Functions","text":"<ul> <li>split_data(features, target, split=0.2): Splits the dataset into training and testing sets, crucial for evaluating the model\u2019s performance.</li> <li>filter_by_coin(coin_name: str, df: pd.DataFrame) -&gt; pd.DataFrame: Filters the dataset to focus on a specific cryptocurrency, allowing for targeted analysis and prediction.</li> <li>get_top_n_features(model_metadata: Dict, n: int): Extracts and ranks the top N most important features from the model, aiding in understanding the key drivers of the predictions.</li> </ul>"},{"location":"ml/overview/","title":"Overview","text":"<p>This documentation details the machine learning (ML) components of the financial dashboard project, which are designed to forecast cryptocurrency prices and analyze various financial indicators. The ML pipeline leverages advanced techniques like Ridge Regression, XGBoost, and ARIMA models.</p> <p>The ML component of this project is focused on predicting cryptocurrency prices and analyzing market trends using sophisticated models and techniques. The pipeline is structured to ensure modularity, scalability, and ease of integration, allowing for continuous improvement and extension.</p>"},{"location":"ml/visualization/","title":"Visualization","text":""},{"location":"ml/visualization/#visualization","title":"Visualization","text":"<p>Visualizations are generated using Plotly, offering interactive and intuitive graphs:</p> <ul> <li>Base Outcome Plot: Displays the actual historical prices of selected cryptocurrencies.</li> <li>Predicted Outcome Plot: Superimposes the model\u2019s predicted prices onto the base outcome, allowing for a clear comparison of model performance.</li> </ul> <p>These visualizations are crucial for both understanding the model\u2019s effectiveness and for communicating results to stakeholders.</p>"},{"location":"ml/workflow/","title":"Workflow","text":""},{"location":"ml/workflow/#workflow","title":"Workflow","text":"<p>Data Preprocessing: </p> <ul> <li>The raw cryptocurrency data undergoes preprocessing, where date and technical features are added. Missing values are imputed, and categorical variables are one-hot encoded, ensuring a clean and structured dataset for modeling.</li> </ul> <p>Model Training:</p> <ul> <li>Ridge Regression and XGBoost models are trained on the processed data. The models are serialized using pickle, enabling easy reuse. Predictions for cryptocurrencies like Bitcoin and Cardano are generated and visualized to compare actual vs. predicted values.</li> </ul> <p>Time Series Forecasting:</p> <ul> <li>ARIMA models forecast future values of key financial metrics for each cryptocurrency over a set number of years. The results are aggregated and saved, providing a long-term view of market trends.</li> </ul> <p>Model Loading and Prediction:</p> <ul> <li>Pre-trained models can be loaded and used to predict future prices based on historical data. This allows for scenario analysis and helps in making informed decisions based on model projections.</li> </ul>"}]}